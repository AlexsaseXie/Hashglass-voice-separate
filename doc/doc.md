# 音频大作业——人声分离器实验报告

石耕源 2015013219 冯玉彤 2015013202 谢运帷 2015013185

---

### 一、人声分离系统工作流程

我们搭建的人声分离器系统，借鉴了作业要求中给出的参考论文1）Music Source Separation Using Stacked Hourglass Networks。此篇文章指导我们完成了人声分离中的重要部分——理由一个堆叠的Hourglass神经网络，将人声和背景音乐的特征从混合的音乐特征中分离出来，得到两幅mask。随后，我们进行这个mask的优化，得到人声和背景音乐两张频幅图特征。最后，我们依靠原音乐的频幅图的复角信息，应用到人声和背景的频幅图上，还原出人声和背景的音频。

1）Stacked Hourglass Network

![network_structure](C:\Users\xyw\Desktop\数字媒体：多媒体\音频大作业\Hourglass\doc\network_structure.png)

上图展示了此篇文章的网络结构：首先，我们的输入是单通道的原音乐的频幅图（1\*512\*64），经过一系列初始卷积核，提取出256张特征图（256\*512\*64），输入到Hourglass模块中。经过一个Hourglass模块，它会分析出两张和输入频幅图相同大小的mask（2\*512\*64），分别对应人声和背景的mask的估计，这个mask和原混合音乐的频谱图的乘积（对应元素相乘）即为整个网络预测的人声/背景的预测频幅图。与此同时，Hourglass模块还会计算出一个和原输入（256\*512\*64）相同大小的特征，输入到下一个Hourglass模块中。

![hourglass](C:\Users\xyw\Desktop\数字媒体：多媒体\音频大作业\Hourglass\doc\hourglass.png)

文章中使用的Hourglass模块就如上图所示，它先通过池化和卷积核将特征图的分辨率降低，再通过上采样将特征还原到与输入相同的大小。整个过程伴随着浅层信息直接和深层信息相加的过程，可以很好的保留原始的特征信息。最后通过1\*1卷积，将输出和下一层Hourglass输入的通道规范化。文中特别提到，网络中没有使用BN和Sigmoid等等规范化函数，这是因为这些函数会让梯度值下降，让整体训练的过程变慢，从结果上来看也不会使准确率提升。

训练过程所需的Loss函数也自然而然地可以给出，即是根据所有Hourglass模块给出的mask得到的预测频谱图和目标频谱图之间的L1范数差异之和。即使在应用中，只有最后一个模块的预测才与最终的预测结果相关，这样的Loss函数还是可以加速训练过程，使网络快速的收敛。

我们的实现基本是按照文中所说的过程实现的。整个网络的训练过程可以在多GPU的机器上进行。



### 输出聚类优化

本部分的目的是对Hourglass网络输出的mask的优化，基于参考论文2，其主要方法是对输入的音乐频谱图进行非负矩阵分解(NMF)以提取特征，针对分离后的特征进行聚类，以达到对人声部分产生更突出且更平滑的输出mask。

1. 针对频谱图NMF

这是传统方法的常用提取特征方法，在多篇论文(,,,)中有所提及。给定频谱图 $X_{C \times M}$ 和特征数 $R$ ，将 $X$ 分解为component矩阵 $W_{C \times R}$ 和 $activation$ 矩阵 $H_{R \times M}$ ，满足 $X=W \cdot H$ 。

2. 提取最突出特征

原频谱图的第 $r$ 层特征( $r \in [1, R]$ ) 表示为矩阵 $X^{(r)}_{C \times M}$ ，其中 $X^{(r)}_{c, m} = W_{c,r} \cdot H_{r, m}$ 。如果认为频谱图矩阵中的每个元素都有一个“最突出的”源(本问题中为人声或背景音乐中的一个源)，则某个元素应该被划分的源由特征矩阵中在该位置的最大的元素决定，即构造矩阵 $F_{C \times M}$ ，满足
$$
F_{c, m} = \arg\max_{1 \leq r \leq R} X^{(r)}_{c, m}
$$

3. 聚类优化



